- [优化程序性能](#优化程序性能)
	- [5.1 优化编译器的能力和局限性](#51-优化编译器的能力和局限性)
	- [5.2 表示程序性能](#52-表示程序性能)
	- [5.4 消除循环的低效率](#54-消除循环的低效率)
	- [5.6 减少不必要的内存引用](#56-减少不必要的内存引用)
	- [5.7 理解现代处理器](#57-理解现代处理器)
		- [5.7.1 整体操作](#571-整体操作)
		- [5.7.2 功能单元的性能](#572-功能单元的性能)
	- [5.8 循环展开](#58-循环展开)
	- [5.9 提高并行性](#59-提高并行性)
		- [5.9.1 多个累积变量](#591-多个累积变量)
		- [5.9.2 重新结合变换](#592-重新结合变换)
	- [5.11 一些限制因素](#511-一些限制因素)
		- [5.11.1 寄存器溢出](#5111-寄存器溢出)
		- [5.11.2 分支预测和预测错误处罚](#5112-分支预测和预测错误处罚)
	- [5.12 理解内存性能](#512-理解内存性能)
		- [5.12.1 加载的性能](#5121-加载的性能)
		- [5.12.2 存储的性能](#5122-存储的性能)
	- [5.13 应用：性能提高技术](#513-应用性能提高技术)
	- [5.14 确认和消除性能瓶颈](#514-确认和消除性能瓶颈)
		- [5.14.1 程序剖析](#5141-程序剖析)
	- [5.15 小结](#515-小结)

# 优化程序性能

**编写高效程序**需要做到以下几点:第一，我们必须选择一组适当的算法和数据结构。第二，我们必须编写出编译器能够有效优化以转换成高效可执行代码的源代码。对于这第二点，理解优化编译器的能力和局限性是很重要的。编写程序方式中看上去只是一点小小的变动，都会引起编译器优化方式很大的变化。有些编程语言比其他语言容易优化。C语言的有些特性，例如执行指针运算和强制类型转换的能力，使得编译器很难对它进行优化。程序员经常能够以一种使编译器更容易产生高效代码的方式来编写他们的程序。第三项技术针对处理运算量特别大的计算，将一个任务分成多个部分，这些部分可以在多核和多处理器的某种组合上并行地计算。我们会把这种性能改进的方法推迟到第12章中去讲。即使是要利用并行性，每个并行的线程都以最高性能执行也是非常重要的，所以无论如何本章所讲的内容也还是有意义的。

**程序优化的第一步就是消除不必要的工作**，让代码尽可能有效地执行所期望的任务。这包括消除不必要的函数调用、条件测试和内存引用。这些优化不依赖于目标机器的任何具体属性。

为了使程序性能最大化，程序员和编译器都需要一个目标机器的模型，指明如何处理指令，以及各个操作的时序特性。了解了处理器的运作，我们就可以进行程序优化的第二步，利用处理器提供的指令级并行能力，同时执行多条指令。

我们以对优化大型程序的问题的讨论来结束这一章。我们描述了代码剖析程序(profi-ler)的使用，代码剖析程序是测量程序各个部分性能的工具。这种分析能够帮助找到代码中低效率的地方，并且确定程序中我们应该着重优化的部分。

## 5.1 优化编译器的能力和局限性

现代编译器运用复杂精细的算法来确定一个程序中计算的是什么值，以及它们是被如何使用的。然后会利用一些机会来简化表达式，在几个不同的地方使用同一个计算，以及降低一个给定的计算必须被执行的次数。大多数编译器，包括GCC，向用户提供了一些对它们所使用的的优化的控制。最简单的控制就是制定优化级别。

编译器必须很小心地对程序只使用安全的优化，也就是说对于程序可能遇到的所有可能的情况，在C语言标准提供的保证之下，优化后得到的程序和未优化的版本有一样的行为。限制编译器只进行安全的优化，消除了造成不希望的运行时行为的一些可能的原因，但是这也意味着程序员必须花费更大的力气写出编译器能够将之转换成有效机器代码的程序。

两个指针可能指向同一个内存位置的情况称为**内存别名使用**。在只执行安全的优化中，编译器必须假设不同的指针可能会指向内存中同一个位置。这造成了一个主要的妨碍优化的因素，这也是可能严重限制编译器产生优化代码机会的程序的一个方面。如果编译器不能确定两个指针是否指向同一个位置，就必须假设什么情况都有可能，这就限制了可能的优化策略。

第二个妨碍优化的因素是**函数调用**。

## 5.2 表示程序性能

我们引入度量标准**每元素的周期数(CPE)**，作为一种表示程序性能并指导我们改进代码的方法。CPE这种度量标准帮助我们在更细节的级别上理解迭代程序的循环性能。这样的度量标准对执行重复计算的程序来说是很适当的。

处理器活动的顺序是由时钟控制的，时钟提供了某个频率的规律信号，通常用千兆赫兹(GHz)，即十亿周期每秒来表示。例如，当表明一个系统有“4GHz”处理器，这表示处理器时钟运行频率为每秒$4×10^9$个周期。每个时钟周期的时间是时钟频率的倒数。通常是以纳秒(1纳秒等于$10^{-9}$秒)或皮秒(1皮秒等于$10^{-12}$秒)为单位的。例如，一个4GHz的时钟其周期为0.25纳秒，或者250皮秒。从程序员的角度来看，用时钟周期来表示度量标准要比用纳秒或皮秒来表示有帮助得多。用时钟周期来表示，度量值表示的是执行了多少条指令，而不是时钟运行得有多快。

## 5.4 消除循环的低效率

代码移动：识别要执行多次（例如在循环里）但是计算结果不会改变的计算。因而可以将计算移动到代码前面不会被多次求值的部分。

## 5.6 减少不必要的内存引用

将运算结果存放在局部变量中，消除每次循环迭代中从内存中读出并将更新值写回的需要。

## 5.7 理解现代处理器

处理器的实际操作与通过观察机器级程序所察觉到的大相径庭。在代码级上，看上去似乎是一次执行一条指令，每条指令都包括从寄存器或内存取值，执行一个操作，并把结果存回到一个寄存器或内存位置。在实际的处理器中，是同时对多条指令求值的，这个现象称为**指令级并行**。在某些设计中，可以有100或更多条指令在处理中。采用一些精细的机制来确保这种并行执行的行为，正好能获得机器级程序要求的顺序语义模型的效果。现代微处理器取得的了不起的功绩之一是:它们采用复杂而奇异的微处理器结构，其中，多条指令可以并行地执行，同时又呈现出一种简单的顺序执行指令的表象。

两种下界描述了程序的最大性能。当一系列操作必须按照严格顺序执行时，就会遇到延迟界限，因为在下一条指令开始之前，这条指令必须结束。当代码中的数据相关限制了处理器利用指令级并行的能力时，延迟界限能够限制程序性能。吞吐量界限刻画了处理器功能单元的原始计算能力。这个界限是程序性能的终极限制。

### 5.7.1 整体操作

**超标量处理器**：可以在每个时钟周期执行多个操作。**乱序处理器**：指令执行的顺序不一定要与它们在机器级程序中的顺序一致。处理器设计有两个主要部分：**指令控制单元和执行单元**。指令控制单元负责从内存中读出指令序列，并根据这些指令序列生成一组针对程序数据的基本操作，执行单元负责执行这些操作。和第4章中研究过的按序流水线相比，乱序处理器需要更大、更复杂的硬件，但是它们能更好地达到更高的指令级并行度。

ICU从指令高速缓存中读取指令，指令高速缓存是一个特殊的高速存储器，它包含最近访问的指令。通常，ICU会在当前正在执行的指令很早之前取指，这样它才有足够的时间对指令译码，并把操作发送到EU。不过，一个问题是当程序遇到分支时，程序有两个可能的前进方向。一种可能会选择分支，控制被传递到分支目标。另一种可能是，不选择分支，控制被传递到指令序列的下一条指令。现代处理器采用了一种称为**分支预测**的技术，处理器会猜测是否会选择分支，同时还预测分支的目标地址。使用投机执行的技术，处理器会开始取出位于它预测的分支会跳到的地方的指令，并对指令译码，甚至在它确定分支预测是否正确之前就开始执行这些操作。如果过后确定分支预测错误，会将状态重新设置到分支点的状态，并开始取出和执行另一个方向上的指令。标记为取指控制的块包括分支预测，以完成确定取哪些指令的任务。

指令译码逻辑接收实际的程序指令，并将它们转换成一组基本操作(有时称为微操作)。每个这样的操作都完成某个简单的计算任务，例如两个数相加，从内存中读数据，或是向内存写数据。对于具有复杂指令的机器，比如x86处理器，一条指令可以被译码成多个操作。关于指令如何被译码成操作序列的细节，不同的机器都会不同，这个信息可谓是高度机密。幸运的是，不需要知道某台机器实现的底层细节，我们也能优化自己的程序。

**EU**接收来自取指单元的操作。通常，每个时钟周期会接收多个操作。这些操作会被分派到一组功能单元中，它们会执行实际的操作。这些功能单元专门用来处理不同类型的操作。

读写内存是由加载和存储单元实现的。加载单元处理从内存读数据到处理器的操作。这个单元有一个加法器来完成地址计算。类似，存储单元处理从处理器写数据到内存的操作。它也有一个加法器来完成地址计算。加载和存储单元通过数据高速缓存来访问内存。数据高速缓存是一个高速存储器，存放着最近访问的数据值。

使用**投机执行技术**对操作求值，但是最终结果不会存放在程序寄存器或数据内存中，直到处理器能确定应该实际执行这些指令。分支操作被送到EU，不是确定分支该往哪里去，而是确定分支预测是否正确。如果预测错误，EU会丢弃分支点之后计算出来的结果。它还会发信号给分支单元，说预测是错误的，并指出正确的分支目的。在这种情况中，分支单元开始在新的位置取指。这样的预测错误会导致很大的性能开销。在可以取出新指令、译码和发送到执行单元之前，要花费一点时间。

在ICU中，**退役单元**记录正在进行的处理，并确保它遵守机器级程序的顺序语义。我们的图中展示了一个寄存器文件，它包含整数、浮点数和最近的SSE和AVX寄存器，是退役单元的一部分，因为退役单元控制这些寄存器的更新。指令译码时，关于指令的信息被放置在一个先进先出的队列中。这个信息会一直保持在队列中，直到发生以下两个结果中的一个。首先，一旦一条指令的操作完成了，而且所有引起这条指令的分支点也都被确认为预测正确，那么这条指令就可以退役了，所有对程序寄存器的更新都可以被实际执行了。另一方面，如果引起该指令的某个分支点预测错误，这条指令会被清空，丢弃所有计算出来的结果。通过这种方法，预测错误就不会改变程序的状态了。

任何对程序寄存器的更新都只会在指令退役时才会发生，只有在处理器能够确信导致这条指令的所有分支都预测正确了，才会这样做。为了加速一条指令到另一条指令的结果的传送，许多此类信息是在执行单元之间交换的。执行单元可以直接将结果发送给彼此。这是4.5.5节中简单处理器设计中采用的数据转发技术的更复杂精细版本。

控制操作数在执行单元间传送的最常见的机制称为**寄存器重命名**。当一条更新寄存器r的指令译码时，产生标记t，得到一个指向该操作结果的唯一的标识符。条目(r，t)被加入到一张表中，该表维护着每个程序寄存器r与会更新该寄存器的操作的标记t之间的关联。当随后以寄存器r作为操作数的指令译码时，发送到执行单元的操作会包含t作为操作数源的值。当某个执行单元完成第一个操作时，会生成一个结果(v，t)，指明标记为t的操作产生值v。所有等待t作为源的操作都能使用v作为源值，这就是一种形式的数据转发。通过这种机制，值可以从一个操作直接转发到另一个操作，而不是写到寄存器文件再读出来，使得第二个操作能够在第一个操作完成后尽快开始。重命名表只包含关于有未进行写操作的寄存器条目。当一条被译码的指令需要寄存器r，而又没有标记与这个寄存器相关联，那么可以直接从寄存器文件中获取这个操作数。有了寄存器重命名，即使只有在处理器确定了分支结果之后才能更新寄存器，也可以预测着执行操作的整个序列。

### 5.7.2 功能单元的性能

Intel Core i7 Haswell参考机的一些算术运算的性能。每个运算都是由以下这些数值来刻画的:一个是延迟(latency)，它表示完成运算所需要的总时间;另一个是发射时间(issue time)，它表示两个连续的同类型的运算之间需要的最小时钟周期数﹔还有一个是容量(capacity)，它表示能够执行该运算的功能单元的数量。

|运算|整数延迟|整数发射|整数容量|浮点数延迟|浮点数发射|浮点数容量|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|加法|1|1|4|3|1|1|
|乘法|3|1|1|5|1|2|
|除法|3~30|3~30|1|3~15|3~15|1|

我们看到，从整数运算到浮点运算，延迟是增加的。还可以看到加法和乘法运算的发射时间都为1，意思是说在每个时钟周期，处理器都可以开始一条新的这样的运算。这种很短的发射时间是通过使用流水线实现的。流水线化的功能单元实现为一系列的阶段，每个阶段完成一部分的运算。例如，一个典型的浮点加法器包含三个阶段(所以有三个周期的延迟):一个阶段处理指数值，一个阶段将小数相加，而另一个阶段对结果进行舍入。算术运算可以连续地通过各个阶段，而不用等待一个操作完成后再开始下一个。只有当要执行的运算是连续的、逻辑上独立的时候，才能利用这种功能。发射时间为1的功能单元被称为完全流水线化的:每个时钟周期可以开始一个新的运算。出现容量大于1的运算是由于有多个功能单元，就如前面所述的参考机一样。

我们还看到，除法器(用于整数和浮点除法，还用来计算浮点平方根)不是完全流水线化的,它的发射时间等于它的延迟。这就意味着在开始一条新运算之前，除法器必须完成整个除法。我们还看到，对于除法的延迟和发射时间是以范围的形式给出的，因为某些被除数和除数的组合比其他的组合需要更多的步骤。除法的长延迟和长发射时间使之成为了一个相对开销很大的运算。

表达发射时间的一种更常见的方法是指明这个**功能单元的最大吞吐量**，定义为发射时间的倒数。一个完全流水线化的功能单元有最大的吞吐量，每个时钟周期一个运算，而发射时间较大的功能单元的最大吞吐量比较小。具有多个功能单元可以进一步提高吞吐量。对一个容量为C，发射时间为Ⅰ的操作来说，处理器可能获得的吞吐量为每时钟周期C/I个操作。比如，我们的参考机可以每个时钟周期执行两个浮点乘法运算。

用CPE值得两个基本界限描述算术运算的延迟、发射时间和容量对函数性能的影响。

|界限|整数+|整数*|浮点数+|浮点数*|
|:-:|:-:|:-:|:-:|:-:|
|延迟|1.00|3.00|3.00|5.00|
|吞吐量|0.50|1.00|1.00|0.50|

延迟界限给出了任何必须按照严格顺序完成合并运算的函数所需要的最小CPE值。根据功能单元产生结果的最大速率，吞吐量界限给出了CPE的最小界限。

## 5.8 循环展开

**循环展开**是一种程序变换，通过增加每次迭代计算的元素的数量，减少循环的迭代次数。循环展开能够从两个方面改进程序的性能。首先，它减少了不直接有助于程序结果的操作的数量，例如循环索引计算和条件分支。第二，它提供了一些方法，可以进一步变化代码，减少整个计算中关键路径上的操作数量。

**kX1展开**：确保第一次循环不会超出数组的界限。对于长度为n的向量，将循环界限设为n-k+1，在循环内对元素i到i+k-1应用运算。每次迭代，循环索引i加k。那么最大循环索引i+k-1会小于n。要使用第二个循环，以每次处理一个元素的方式处理向量的最后几个元素。这个循环体将会执行0~k-1次。

## 5.9 提高并行性

### 5.9.1 多个累积变量

我们可以将多个累积变量变换归纳为将循环展开k次，以及并行累积k个值，得到kXk循环展开。

只有保持能够执行该操作的所有功能单元的流水线都是满的，程序才能达到这个操作的吞吐量界限。对延迟为L，容量为C的操作而言，这就要求循环展开因子k>=C*L.

### 5.9.2 重新结合变换

重新结合变换能够减少计算中关键路径上操作的数量，通过更好地利用功能单元的流水线能力得到更好的性能。大多数编译器不会尝试对浮点运算做重新结合，因为这些运算不保证是可结合的。通常，我们发现循环展开和并行地累积在多个值中，是提高程序性能的更可靠的方法。

## 5.11 一些限制因素

在一个程序的数据流图表示中，关键路径指明了执行该程序所需时间的一个基本的下界。也就是说，如果程序中有某条数据相关链，这条链上的所有延迟之和等于T，那么这个程序至少需要T个周期才能执行完。

功能单元的吞吐量界限也是程序执行时间的一个下界。也就是说，假设一个程序一共需要N个某种运算的计算，而微处理器只有C个能执行这个操作的功能单元，并且这些单元的发射时间为I。那么，这个程序的执行至少需要N·I/C个周期。

### 5.11.1 寄存器溢出

循环并行性的好处受汇编代码描述计算的能力限制。如果我们的并行度p超过了可用的寄存器数量，那么编译器会诉诸溢出，将某些临时值存放到内存中，通常是在运行时堆栈上分配空间。对这种循环展开的增加没有改善CPE，有些甚至会变差。

一旦编译器必须要诉诸寄存器溢出，那么维护多个累积变量的优势就很可能消失。幸运的是，x86-64有足够多的寄存器，大多数循环在出现寄存器溢出之前就将达到吞吐量限制。

### 5.11.2 分支预测和预测错误处罚

在3.6.6节中通过实验证明，当分支预测逻辑不能正确预测一个分支是否要跳转的时候，条件分支可能会招致很大的预测错误处罚。

现代处理器的工作远超前于当前正在执行的指令，从内存读新指令，译码指令，以确定在什么操作数上执行什么操作。只要指令遵循的是一种简单的顺序，那么这种指令流水线化就能很好地工作。当遇到分支的时候，处理器必须猜测分支该往哪个方向走。对于条件转移的情况，这意味着要预测是否会选择分支。对于像间接跳转(跳转到由一个跳转表条目指定的地址)或过程返回这样的指令，这意味着要预测目标地址。在这里，我们主要讨论条件分支。

在一个使用投机执行的处理器中，处理器会开始执行预测的分支目标处的指令。它会避免修改任何实际的寄存器或内存位置，直到确定了实际的结果。如果预测正确，那么处理器就会“提交”投机执行的指令的结果，把它们存储到寄存器或内存。如果预测错误，处理器必须丢弃掉所有投机执行的结果，在正确的位置，重新开始取指令的过程。这样做会引起预测错误处罚，因为在产生有用的结果之前，必须重新填充指令流水线。

在3.6.6节中我们看到，最近的x86处理器(包含所有可以执行x86-64程序的处理器)有条件传送指令。在编译条件语句和表达式的时候，GCC能产生使用这些指令的代码，而不是更传统的基于控制的条件转移的实现。翻译成条件传送的基本思想是计算出一个条件表达式或语句两个方向上的值，然后用条件传送选择期望的值。在4.5.7节中我们看到，条件传送指令可以被实现为普通指令流水线化处理的一部分。没有必要猜测条件是否满足，因此猜测错误也没有处罚。

那么一个C语言程序员怎么能够保证分支预测处罚不会阻碍程序的效率呢?对于参考机来说，预测错误处罚是19个时钟周期，赌注很高。对于这个问题没有简单的答案，但是下面的**通用原则**是可用的。

1. 不要过分关心可预测的分支

我们已经看到错误的分支预测的影响可能非常大，但是这并不意味着所有的程序分支都会减缓程序的执行。实际上，现代处理器中的分支预测逻辑非常善于辨别不同的分支指令的有规律的模式和长期的趋势。

2. 书写适合用条件传送实现的代码

分支预测只对有规律的模式可行。程序中的许多测试是完全不可预测的，依赖于数据的任意特性，例如一个数是负数还是正数。对于这些测试，分支预测逻辑会处理得很糟糕。对于本质上无法预测的情况，如果编译器能够产生使用条件数据传送而不是使用条件控制转移的代码，可以极大地提高程序的性能。这不是C语言程序员可以直接控制的，但是有些表达条件行为的方法能够更直接地被翻译成条件传送，而不是其他操作。

我们发现GCC能够为以一种更“功能性的”风格书写的代码产生条件传送，在这种风格的代码中，我们用条件操作来计算值，然后用这些值来更新程序状态，这种风格对立于一种更“命令式的”风格，这种风格中，我们用条件语句来有选择地更新程序状态。

这两种风格也没有严格的规则，我们用一个例子来说明。假设给定两个整数数组a和b，对于每个位置i，我们想将a[i\]设置为a[i\]和b[i\]中较小的那一个，而将b[i\]设置为两者中较大的那一个。

用功能式的风格实现
```C
for (i = 0; i < n; i++) {
	long min = a[i] < b[i] ? a[i] : b[i];
	long max = a[i] < b[i] ? b[i] : a[i];
	a[i] = min;    b[i] = max;
}
```

用命令式的风格实现
```C
for (i = 0; i < n; i++) {
	if (a[i] > b[i]) {
		long t = a[i];
		a[i] = b[i];
		b[i] = t;
	}
}
```

## 5.12 理解内存性能

到目前为止我们写的所有代码，以及运行的所有测试，只访问相对比较少量的内存。所有的现代处理器都包含一个或多个高速缓存存储器，以对这样少量的存储器提供快速的访问。本节会进一步研究涉及加载(从内存读到寄存器)和存储(从寄存器写到内存)操作的程序的性能，只考虑所有的数据都存放在高速缓存中的情况。在第6章，我们会更详细地探究高速缓存是如何工作的，它们的性能特性，以及如何编写充分利用高速缓存的代码。

现代处理器有专门的功能单元来执行加载和存储操作，这些单元有内部的缓冲区来保存未完成的内存操作请求集合。每个这样的单元通常可以每个时钟周期开始一个操作。

### 5.12.1 加载的性能

一个包含加载操作的程序的性能既依赖于流水线的能力，也依赖于加载单元的延迟。在参考机上运行合并操作的实验中，我们看到除了使用SIMD操作时以外，对任何数据类型组合和合并操作来说，CPE从没有到过0.50以下。一个制约示例的CPE的因素是，对于每个被计算的元素，所有的示例都需要从内存读一个值。对两个加载单元而言，其每个时钟周期只能启动一条加载操作，所以CPE不可能小于0.50。对于每个被计算的元素必须加载k个值的应用，我们不可能获得低于k/2的CPE。

### 5.12.2 存储的性能

与加载操作一样，在大多数情况中，存储操作能够在完全流水线化的模式中工作，每个周期开始一条新的存储。

与到目前为止我们已经考虑过的其他操作不同，存储操作并不影响任何寄存器值。因此，就其本性来说，一系列存储操作不会产生数据相关。只有加载操作会受存储操作结果的影响，因为只有加载操作能从由存储操作写的那个位置读回值。

**写/读相关**：一个内存读的结果依赖于一个最近的内存写。

存储单元包含一个存储缓冲区，它包含已经被发射到存储单元而又还没有完成的存储操作的地址和数据，这里的完成包括更新数据高速缓存。提供这样一个缓冲区，使得一系列存储操作不必等待每个操作都更新高速缓存就能够执行。当一个加载操作发生时，它必须检查存储缓冲区中的条目，看有没有地址相匹配。如果有地址相匹配(意味着在写的字节与在读的字节有相同的地址)，它就取出相应的数据条目作为加载操作的结果。

内存操作的实现包括许多细微之处。对于寄存器操作，在指令被译码成操作的时候，处理器就可以确定哪些指令会影响其他哪些指令。另一方面，对于内存操作，只有到计算出加载和存储的地址被计算出来以后，处理器才能确定哪些指令会影响其他的哪些。高效地处理内存操作对许多程序的性能来说至关重要。内存子系统使用了很多优化，例如当操作可以独立地进行时,就利用这种潜在的并行性

## 5.13 应用：性能提高技术

优化程序性能的基本策略：

  * 1):高级设计。为遇到的问题选择适当的算法和数据结构。要特别警觉，避免使用那些会渐进地产生糟糕性能的算法或编码技术

  * 2):基本编码原则。避免限制优化的因素，这样编译器就能产生高效的代码。消除连续的函数调用。在可能时，将计算移到循环外。考虑有选择地妥协程序的模块性以获得更大的效率；消除不必要的内存引用。引入临时变量来保存中间结果。只有在最后的值计算出来时,才将结果存放到数组或全局变量中。

  * 3）低级优化。结构化代码以利用硬件功能。展开循环，降低开销，并且使得进一步的优化成为可能；通过使用例如多个累积变量和重新结合等技术，找到方法提高指令级并行；用功能性的风格重写条件操作,使得编译采用条件数据传送。

要警惕，在为了提高效率重写程序时避免引入错误。在引入新变量、改变循环边界和使得代码整体上更复杂时，很容易犯错误。一项有用的技术是在优化函数时，用检查代码来测试函数的每个版本，以确保在这个过程没有引入错误。检查代码对函数的新版本实施一系列的测试，确保它们产生与原来一样的结果。对于高度优化的代码，这组测试情况必须变得更加广泛，因为要考虑的情况也更多。例如，使用循环展开的检查代码需要测试许多不同的循环界限，保证它能够处理最终单步迭代所需要的所有不同的可能的数字。

## 5.14 确认和消除性能瓶颈

至此，我们只考虑了优化小的程序，在这样的小程序中有一些很明显限制性能的地方，因此应该是集中注意力对它们进行优化。在处理大程序时，连知道应该优化什么地方都是很难的。本节会描述如何使用代码剖析程序，这是在程序执行时收集性能数据的分析工具。还展示了一个系统优化的通用原则，称为Amdahl定律。

### 5.14.1 程序剖析

程序剖析运行程序的一个版本，其中插入了工具代码，以确定程序的各个部分需要多少时间。这对于确认程序中我们需要集中注意力优化的部分是很有用的。剖析的一个有力之处在于可以在现实的基准数据上运行实际程序的同时，进行剖析。

Unix系统提供了一个**剖析程序GPROF**。这个程序产生两种形式的信息。首先，它确定程序中每个函数花费了多少CPU时间。其次，它计算每个函数被调用的次数，以执行调用的函数来分类。这两种形式的信息都非常有用。这些计时给出了不同函数在确定整体运行时间中的相对重要性。调用信息使得我们能理解程序的动态行为。

**GPROF使用步骤**，运行C程序prog.c，运行时命令行参数为file.txt

1. 程序必须为剖析而编译和链接。使用GCC(以及其他C编译器)，就是在命令行上简单地包括运行时标志“-pg”。确保编译器不通过内联替换来尝试执行任何优化是很重要的，否则就可能无法正确刻画函数调用。我们使用优化标志-og，以保证能正确跟踪函数调用。`linux> gcc -Og -pg prog.c -o prog`

2. 然后程序像往常一样执行：`linux> ./prog file.txt`它运行得会比正常时稍微慢一点(大约慢2倍)，不过除此之外唯一的区别就是它产生了一个文件 gmon.out。

3. 调用GPROF来分析gomn.out中的数据。`linux> gprof prog`

剖析报告的第一部分列出了执行各个函数花费的时间，按照降序排列。每一行代表对某个函数的所有调用所花费的时间。第一列表明花费在这个函数上的时间占整个时间的百分比。第二列显示的是直到这一行并包括这一行的函数所花费的累计时间。第三列显示的是花费在这个函数上的时间，而第四列显示的是它被调用的次数(递归调用不计算在内)。

剖析报告的地热部分是函数的调用历史。这个历史既显示了调用函数xxx的函数，也显示了它调用的函数。根据这个调用信息，我们通常可以推断出关于程序行为的有用信息。

GPROF有些**属性**值得注意:

* 计时不是很准确。它的计时基于一个简单的间隔计数机制，编译过的程序为每个函数维护一个计数器，记录花费在执行该函数上的时间。操作系统使得每隔某个规则的时间间隔δ，程序被中断一次。δ的典型值的范围为1.0～10.0毫秒。当中断发生时，它会确定程序正在执行什么函数，并将该函数的计数器值增加δ。当然，也可能这个函数只是刚开始执行，而很快就会完成，却赋给它从上次中断以来整个的执行花费。在两次中断之间也可能运行其他某个程序，却因此根本没有计算花费。对于运行时间较长的程序，这种机制工作得相当好。从统计上来说，应该根据花费在执行函数上的相对时间来计算每个函数的花费。不过，对于那些运行时间少于1秒的程序来说,得到的统计数字只能看成是粗略的估计值。

* 假设没有执行内联替换，则调用信息相当可靠。编译过的程序为每对调用者和被调用者维护一个计数器。每次调用一个过程时，就会对适当的计数器加1。

* 默认情况下，不会显示对库函数的计时。相反，库函数的时间都被计算到调用它们的函数的时间中。

## 5.15 小结

虽然关于代码优化的大多数论述都描述了编译器是如何能生成高效代码的，但是应用程序员有很多方法来协助编译器完成这项任务。没有任何编译器能用一个好的算法或数据结构代替低效率的算法或数据结构，因此程序设计的这些方面仍然应该是程序员主要关心的。我们还看到妨碍优化的因素，例如内存别名使用和过程调用，严重限制了编译器执行大量优化的能力。同样，程序员必须对消除这些妨碍优化的因素负主要的责任。这些应该被看作好的编程习惯的一部分，因为它们可以用来消除不必要的工作。

基本级别之外调整性能需要一些对处理器微体系结构的理解，描述处理器用来实现它的指令集体系结构的底层机制。对于乱序处理器的情况，只需要知道一些关于操作、容量、延迟和功能单元发射时间的信息，就能够基本地预测程序的性能了。

我们研究了一系列技术，包括循环展开、创建多个累积变量和重新结合，它们可以利用现代处理器提供的指令级并行。随着对优化的深入，研究产生的汇编代码以及试着理解机器如何执行计算变得重要起来。确认由程序中的数据相关决定的关键路径，尤其是循环的不同迭代之间的数据相关，会收获良多。我们还可以根据必须要计算的操作数量以及执行这些操作的功能单元的数量和发射时间，计算一个计算的吞吐量界限。

包含条件分支或与内存系统复杂交互的程序，比我们最开始考虑的简单循环程序，更难以分析和优化。基本策略是使分支更容易预测，或者使它们很容易用条件数据传送来实现。我们还必须注意存储和加载操作。将数值保存在局部变量中，使得它们可以存放在寄存器中，这会很有帮助。

当处理大型程序时，将注意力集中在最耗时的部分变得很重要。代码剖析程序和相关的工具能帮助我们系统地评价和改进程序性能。我们描述了GPROF，一个标准的Unix剖析工具。还有更加复杂完善的剖析程序可用，例如Intel的VTUNE程序开发系统，还有Linux系统基本上都有的VALGRIND。这些工具可以在过程级分解执行时间，估计程序每个基本块的性能(基本块是内部没有控制转移的指令序列,因此基本块总是整个被执行的)。
